{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3d_lung_seg_v1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNF1ALUaGAlEq7+VB6NG/DR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-mostafa-a/lung-cancer-segmentation/blob/master/3d_lung_seg_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUKjlJ3XUIf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9aaa8bca-8c3a-4c66-951a-aeae9e9ae370"
      },
      "source": [
        "! git clone https://github.com/circulosmeos/gdown.pl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gdown.pl'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "Unpacking objects:   1% (1/92)   \rUnpacking objects:   2% (2/92)   \rUnpacking objects:   3% (3/92)   \rUnpacking objects:   4% (4/92)   \rUnpacking objects:   5% (5/92)   \rUnpacking objects:   6% (6/92)   \rUnpacking objects:   7% (7/92)   \rUnpacking objects:   8% (8/92)   \rUnpacking objects:   9% (9/92)   \rUnpacking objects:  10% (10/92)   \rUnpacking objects:  11% (11/92)   \rUnpacking objects:  13% (12/92)   \rUnpacking objects:  14% (13/92)   \rUnpacking objects:  15% (14/92)   \rUnpacking objects:  16% (15/92)   \rUnpacking objects:  17% (16/92)   \rUnpacking objects:  18% (17/92)   \rUnpacking objects:  19% (18/92)   \rUnpacking objects:  20% (19/92)   \rUnpacking objects:  21% (20/92)   \rUnpacking objects:  22% (21/92)   \rUnpacking objects:  23% (22/92)   \rUnpacking objects:  25% (23/92)   \rUnpacking objects:  26% (24/92)   \rUnpacking objects:  27% (25/92)   \rUnpacking objects:  28% (26/92)   \rUnpacking objects:  29% (27/92)   \rUnpacking objects:  30% (28/92)   \rUnpacking objects:  31% (29/92)   \rUnpacking objects:  32% (30/92)   \rUnpacking objects:  33% (31/92)   \rUnpacking objects:  34% (32/92)   \rUnpacking objects:  35% (33/92)   \rUnpacking objects:  36% (34/92)   \rUnpacking objects:  38% (35/92)   \rUnpacking objects:  39% (36/92)   \rremote: Total 92 (delta 0), reused 0 (delta 0), pack-reused 92\u001b[K\n",
            "Unpacking objects:  40% (37/92)   \rUnpacking objects:  41% (38/92)   \rUnpacking objects:  42% (39/92)   \rUnpacking objects:  43% (40/92)   \rUnpacking objects:  44% (41/92)   \rUnpacking objects:  45% (42/92)   \rUnpacking objects:  46% (43/92)   \rUnpacking objects:  47% (44/92)   \rUnpacking objects:  48% (45/92)   \rUnpacking objects:  50% (46/92)   \rUnpacking objects:  51% (47/92)   \rUnpacking objects:  52% (48/92)   \rUnpacking objects:  53% (49/92)   \rUnpacking objects:  54% (50/92)   \rUnpacking objects:  55% (51/92)   \rUnpacking objects:  56% (52/92)   \rUnpacking objects:  57% (53/92)   \rUnpacking objects:  58% (54/92)   \rUnpacking objects:  59% (55/92)   \rUnpacking objects:  60% (56/92)   \rUnpacking objects:  61% (57/92)   \rUnpacking objects:  63% (58/92)   \rUnpacking objects:  64% (59/92)   \rUnpacking objects:  65% (60/92)   \rUnpacking objects:  66% (61/92)   \rUnpacking objects:  67% (62/92)   \rUnpacking objects:  68% (63/92)   \rUnpacking objects:  69% (64/92)   \rUnpacking objects:  70% (65/92)   \rUnpacking objects:  71% (66/92)   \rUnpacking objects:  72% (67/92)   \rUnpacking objects:  73% (68/92)   \rUnpacking objects:  75% (69/92)   \rUnpacking objects:  76% (70/92)   \rUnpacking objects:  77% (71/92)   \rUnpacking objects:  78% (72/92)   \rUnpacking objects:  79% (73/92)   \rUnpacking objects:  80% (74/92)   \rUnpacking objects:  81% (75/92)   \rUnpacking objects:  82% (76/92)   \rUnpacking objects:  83% (77/92)   \rUnpacking objects:  84% (78/92)   \rUnpacking objects:  85% (79/92)   \rUnpacking objects:  86% (80/92)   \rUnpacking objects:  88% (81/92)   \rUnpacking objects:  89% (82/92)   \rUnpacking objects:  90% (83/92)   \rUnpacking objects:  91% (84/92)   \rUnpacking objects:  92% (85/92)   \rUnpacking objects:  93% (86/92)   \rUnpacking objects:  94% (87/92)   \rUnpacking objects:  95% (88/92)   \rUnpacking objects:  96% (89/92)   \rUnpacking objects:  97% (90/92)   \rUnpacking objects:  98% (91/92)   \rUnpacking objects: 100% (92/92)   \rUnpacking objects: 100% (92/92), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqv2yWq8UMTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c40430d3-7d5d-4c98-ca8c-4b36ea8b0d3a"
      },
      "source": [
        "!./gdown.pl/gdown.pl https://drive.google.com/file/d/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot open cookies file ‘gdown.cookie.temp’: No such file or directory\n",
            "--2020-07-06 09:14:22--  https://docs.google.com/uc?id=1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W&export=download\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.184.138, 64.233.184.102, 64.233.184.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.184.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘gdown.20200706091422.977766455457644’\n",
            "\n",
            "     0K                                     35.4M=0s\n",
            "\n",
            "2020-07-06 09:14:22 (35.4 MB/s) - ‘gdown.20200706091422.977766455457644’ saved [3259]\n",
            "\n",
            "--2020-07-06 09:14:22--  https://docs.google.com/uc?export=download&confirm=h9yH&id=1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.184.102, 64.233.184.101, 64.233.184.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.184.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e=download [following]\n",
            "--2020-07-06 09:14:22--  https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e=download\n",
            "Resolving doc-0k-c4-docs.googleusercontent.com (doc-0k-c4-docs.googleusercontent.com)... 74.125.133.132, 2a00:1450:400c:c07::84\n",
            "Connecting to doc-0k-c4-docs.googleusercontent.com (doc-0k-c4-docs.googleusercontent.com)|74.125.133.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=kvhtbf6odt4ng&continue=https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e%3Ddownload&hash=st0ldftp029i1tlsd2l6ekqnq4hb527i [following]\n",
            "--2020-07-06 09:14:23--  https://docs.google.com/nonceSigner?nonce=kvhtbf6odt4ng&continue=https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e%3Ddownload&hash=st0ldftp029i1tlsd2l6ekqnq4hb527i\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.184.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e=download&nonce=kvhtbf6odt4ng&user=15121996826791716971Z&hash=jpl8alc3fph0tbvc9ur3p51rquvjv9ed [following]\n",
            "--2020-07-06 09:14:23--  https://doc-0k-c4-docs.googleusercontent.com/docs/securesc/2vrb4s6ks2rdnq8rcqogvaig6g3cls3o/mcovog4ep3c33ld80nhme5o1l1gh3aam/1594026825000/14246493544528845039/15121996826791716971Z/1s3X1zJMGIQVN5V9zD8hcKLlnIz96pH-W?e=download&nonce=kvhtbf6odt4ng&user=15121996826791716971Z&hash=jpl8alc3fph0tbvc9ur3p51rquvjv9ed\n",
            "Connecting to doc-0k-c4-docs.googleusercontent.com (doc-0k-c4-docs.googleusercontent.com)|74.125.133.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-tar]\n",
            "Saving to: ‘gdown.20200706091422.977766455457644’\n",
            "\n",
            "     0K ........ ........ ........ ........ 77.2M\n",
            " 32768K ........ ........ ........ ........  122M\n",
            " 65536K ........ ........ ........ ........  118M\n",
            " 98304K ........ ........ ........ ........ 63.9M\n",
            "131072K ........ ........ ........ ........  181M\n",
            "163840K ........ ........ ........ ........  128M\n",
            "196608K ........ ........ ........ ........  115M\n",
            "229376K ........ ........ ........ ........  133M\n",
            "262144K ........ ........ ........ ........  131M\n",
            "294912K ........ ........ ........ ........  159M\n",
            "327680K ........ ........ ........ ........ 96.6M\n",
            "360448K ........ ........ ........ ........  159M\n",
            "393216K ........ ........ ........ ........  117M\n",
            "425984K ........ ........ ........ ........  174M\n",
            "458752K ........ ........ ........ ........  152M\n",
            "491520K ........ ........ ........ ........  132M\n",
            "524288K ........ ........ ........ ........  138M\n",
            "557056K ........ ........ ........ ........  145M\n",
            "589824K ........ ........ ........ ........  129M\n",
            "622592K ........ ........ ........ ........  104M\n",
            "655360K ........ ........ ........ ........  178M\n",
            "688128K ........ ........ ........ ........  132M\n",
            "720896K ........ ........ ........ ........  142M\n",
            "753664K ........ ........ ........ ........  167M\n",
            "786432K ........ ........ ........ ........ 79.3M\n",
            "819200K ........ ........ ........ ........  187M\n",
            "851968K ........ ........ ........ ........  151M\n",
            "884736K ........ ........ ........ ........ 91.3M\n",
            "917504K ........ ........ ........ ........ 73.4M\n",
            "950272K ........ ........ ........ ........  166M\n",
            "983040K ........ ........ ........ ........  134M\n",
            "1015808K ........ ........ ........ ........  130M\n",
            "1048576K ........ ........ ........ ........  100M\n",
            "1081344K ........ ........ ........ ........  240M\n",
            "1114112K ........ ........ ........ ........  222M\n",
            "1146880K ........ ........ ........ ........  148M\n",
            "1179648K ........ ........ ........ ........  285M\n",
            "1212416K ........ ........ ........ ........  170M\n",
            "1245184K ........ ........ ........ ........  199M\n",
            "1277952K ........ ........ ........ ........  238M\n",
            "1310720K ........ ........ ........ ........  159M\n",
            "1343488K ........ ........ ........ ........  230M\n",
            "1376256K ........ ........ ........ ........  191M\n",
            "1409024K ........ ........ ........ ........ 96.6M\n",
            "1441792K ........ ........ ........ ........ 99.9M\n",
            "1474560K ........ ........ ........ ........  101M\n",
            "1507328K ........ ........ ........ ........  204M\n",
            "1540096K ........ ........ ........ ........  236M\n",
            "1572864K ........ ........ ........ ........ 77.1M\n",
            "1605632K ........ ........ ........ ........  107M\n",
            "1638400K ........ ........ ........ ........ 93.5M\n",
            "1671168K ........ ........ ........ ........ 72.5M\n",
            "1703936K ........ ........ ........ ........ 60.5M\n",
            "1736704K ........ ........ ........ ........ 80.4M\n",
            "1769472K ........ ........ ........ ........  121M\n",
            "1802240K ........ ........ ........ ........ 91.3M\n",
            "1835008K ........ ........ ........ ........  147M\n",
            "1867776K ........ ........ ........ ........ 31.6M\n",
            "1900544K ........ ........ ........ ........ 53.5M\n",
            "1933312K ........ ........ ........ ........ 35.4M\n",
            "1966080K ........ ........ ........ ........ 5.60M\n",
            "1998848K ........ ........ ........ ........ 26.9M\n",
            "2031616K ........ ........ ........ ........ 24.7M\n",
            "2064384K ........ ........ ........ ........ 25.6M\n",
            "2097152K ........ ........ ........ ........ 61.1M\n",
            "2129920K ........ ........ ........ ........ 90.5M\n",
            "2162688K ........ ........ ........ ........  147M\n",
            "2195456K ........ ........ ........ ........  133M\n",
            "2228224K ........ ........ ........ ........  150M\n",
            "2260992K ........ ........ ........ ........  161M\n",
            "2293760K ........ ........ ........ ........  129M\n",
            "2326528K ........ ........ ........ ........  151M\n",
            "2359296K ........ ........ ........ ........  210M\n",
            "2392064K ........ ........ ........ ........  208M\n",
            "2424832K ........ ........ ........ ........ 52.0M\n",
            "2457600K ........ ........ ........ ........ 12.5M\n",
            "2490368K ........ ........ ........ ........  135M\n",
            "2523136K ........ ........ ........ ........ 56.6M\n",
            "2555904K ........ ........ ........ ........ 89.0M\n",
            "2588672K ........ ........ ........ ........ 62.9M\n",
            "2621440K ........ ........ ........ ........ 31.3M\n",
            "2654208K ........ ........ ........ ........ 31.1M\n",
            "2686976K ........ ........ ........ ........ 17.9M\n",
            "2719744K ........ ........ ........ ........ 30.2M\n",
            "2752512K ........ ........ ........ ........ 56.9M\n",
            "2785280K ........ ........ ........ ........ 45.7M\n",
            "2818048K ........ ........ ........ ........ 33.6M\n",
            "2850816K ........ ........ ........ ........ 16.5M\n",
            "2883584K ........ ........ ........ ........ 53.7M\n",
            "2916352K ........ ........ ........ ........ 36.8M\n",
            "2949120K ........ ........ ........ ........ 24.8M\n",
            "2981888K ........ ........ ........ ........ 32.2M\n",
            "3014656K ........ ........ ........ ........ 49.4M\n",
            "3047424K ........ ........ ........ ........ 35.4M\n",
            "3080192K ........ ........ ........ ........ 24.4M\n",
            "3112960K ........ ........ ........ ........ 80.1M\n",
            "3145728K ........ ........ ........ ........ 29.7M\n",
            "3178496K ........ ........ ........ ........ 33.3M\n",
            "3211264K ........ ........ ........ ........ 36.0M\n",
            "3244032K ........ ........ ........ ........ 44.4M\n",
            "3276800K ........ ........ ........ ........ 36.2M\n",
            "3309568K ........ ........ ........ ........ 46.5M\n",
            "3342336K ........ ........ ........ ........ 34.8M\n",
            "3375104K ........ ........ ........ ........ 47.0M\n",
            "3407872K ........ ........ ........ ........ 41.1M\n",
            "3440640K ........ ........ ........ ........ 28.4M\n",
            "3473408K ........ ........ ........ ........ 51.1M\n",
            "3506176K ........ ........ ........ ........ 35.6M\n",
            "3538944K ........ ........ ........ ........ 52.7M\n",
            "3571712K ........ ........ ........ ........ 37.5M\n",
            "3604480K ........ ........ ........ ........ 26.5M\n",
            "3637248K ........ ........ ........ ........ 43.4M\n",
            "3670016K ........ ........ ........ ........ 31.5M\n",
            "3702784K ........ ........ ........ ........ 41.4M\n",
            "3735552K ........ ........ ........ ........ 47.6M\n",
            "3768320K ........ ........ ........ ........ 41.8M\n",
            "3801088K ........ ........ ........ ........ 30.3M\n",
            "3833856K ........ ........ ........ ........ 44.4M\n",
            "3866624K ........ ........ ........ ........ 35.6M\n",
            "3899392K ........ ........ ........ ........ 36.1M\n",
            "3932160K ........ ........ ........ ........ 46.5M\n",
            "3964928K ........ ........ ........ ........ 32.9M\n",
            "3997696K ........ ........ ........ ........ 73.0M\n",
            "4030464K ........ ........ ........ ........ 38.3M\n",
            "4063232K ........ ........ ........ ........ 49.2M\n",
            "4096000K ........ ........ ........ ........ 56.1M\n",
            "4128768K ........ ........ ........ ........ 40.9M\n",
            "4161536K ........ ........ ........ ........ 20.7M\n",
            "4194304K ........ ........ ........ ........ 29.6M\n",
            "4227072K ........ ........ ........ ........ 34.3M\n",
            "4259840K ........ ........ ........ ........ 36.3M\n",
            "4292608K ........ ........ ........ ........ 46.8M\n",
            "4325376K ........ ........ ........ ........ 37.8M\n",
            "4358144K ........ ........ ........ ........ 40.6M\n",
            "4390912K ........ ........ ........ ........ 34.5M\n",
            "4423680K ........ ........ ........ ........ 39.5M\n",
            "4456448K ........ ........ ........ ........ 27.0M\n",
            "4489216K ........ ........ ........ ........ 53.8M\n",
            "4521984K ........ ........ ........ ........ 40.8M\n",
            "4554752K ........ ........ ........ ........ 46.7M\n",
            "4587520K ........ ........ ........ ........ 33.3M\n",
            "4620288K ........ ........ ........ ........ 36.0M\n",
            "4653056K ........ ........ ........ ........ 32.5M\n",
            "4685824K ........ ........ ........ ........ 43.8M\n",
            "4718592K ........ ........ ........ ........ 42.7M\n",
            "4751360K ........ ........ ........ ........ 41.1M\n",
            "4784128K ........ ........ ........ ........ 43.6M\n",
            "4816896K ........ ........ ........ ........ 31.1M\n",
            "4849664K ........ ........ ........ ........ 40.2M\n",
            "4882432K ........ ........ ........ ........ 22.8M\n",
            "4915200K ........ ........ ........ ........ 48.3M\n",
            "4947968K ........ ........ ........ ........ 45.9M\n",
            "4980736K ........ ........ ........ ........ 44.4M\n",
            "5013504K ........ ........ ........ ........ 48.6M\n",
            "5046272K ........ ........ ........ ........ 86.5M\n",
            "5079040K ........ ........ ........ ........ 69.2M\n",
            "5111808K ........ ........ ........ ........ 6.80M\n",
            "5144576K ........ ........ ........ ........ 30.3M\n",
            "5177344K ........ ........ ........ ........ 20.1M\n",
            "5210112K ........ ........ ........ ........ 28.3M\n",
            "5242880K ........ ........ ........ ........ 52.9M\n",
            "5275648K ........ ........ ........ ........  132M\n",
            "5308416K ........ ........ ........ ........  204M\n",
            "5341184K ........ ........ ........ ........  199M\n",
            "5373952K ........ ........ ........ ........ 95.2M\n",
            "5406720K ........ ........ ........ ........  153M\n",
            "5439488K ........ ........ ........ ........  230M\n",
            "5472256K ........ ........ ........ ........  166M\n",
            "5505024K ........ ........ ........ ........  180M\n",
            "5537792K ........ ........ ........ ........  137M\n",
            "5570560K ........ ........ ........ ........ 30.3M\n",
            "5603328K ........ ........ ........ ........  244M\n",
            "5636096K ........ ........ ........ ........  248M\n",
            "5668864K ........ ........ ........ ........ 29.3M\n",
            "5701632K ........ ........ ........ ........ 29.2M\n",
            "5734400K ........ ........ ........ ........ 31.1M\n",
            "5767168K ........ ........ ........ ........ 27.2M\n",
            "5799936K ........ ........ ........ ........ 18.6M\n",
            "5832704K ........ ........ ........ ........ 37.5M\n",
            "5865472K ........ ........ ........ ........ 29.2M\n",
            "5898240K ........ ........ ........ ........ 23.9M\n",
            "5931008K ........ ........ ........ ........ 48.9M\n",
            "5963776K ........ ........ ........ ........ 34.6M\n",
            "5996544K ........ ........ ........ ........ 32.6M\n",
            "6029312K ........ ........ ........ ........  115M\n",
            "6062080K ........ ........ ........ ........ 65.9M\n",
            "6094848K ........ ........ ........ ........ 45.9M\n",
            "6127616K ........ ........ ........ ........ 33.0M\n",
            "6160384K ........ ........ ........ ........ 22.9M\n",
            "6193152K ........ ........ ........ ........ 30.1M\n",
            "6225920K ........ ........ ........ ........ 55.8M\n",
            "6258688K ........ ........ ........ ........ 52.2M\n",
            "6291456K ........ ........ ........ ........ 28.8M\n",
            "6324224K ........ ........ ........ ........ 28.8M\n",
            "6356992K ........ ........ ........ ........ 35.1M\n",
            "6389760K ........ ........ ........ ........ 45.0M\n",
            "6422528K ........ ........ ........ ........ 38.0M\n",
            "6455296K ........ ........ ........ ........ 32.2M\n",
            "6488064K ........ ........ ........ ........ 45.7M\n",
            "6520832K ........ ........ ........ ........ 34.1M\n",
            "6553600K ........ ........ ........ ........ 38.7M\n",
            "6586368K ........ ........ ........ ........ 45.7M\n",
            "6619136K ........ ........ ........ ........ 30.7M\n",
            "6651904K ........ ........ ........ ........ 45.0M\n",
            "6684672K ........ ........ ........ ........ 12.0M\n",
            "6717440K ........ ........ ........ ........ 37.3M\n",
            "6750208K ........ ........ ........ ........ 29.0M\n",
            "6782976K ........ ........ ........ ........ 36.0M\n",
            "6815744K ........ ........ ........ ........ 29.4M\n",
            "6848512K ........ ........ ........ ........ 59.3M\n",
            "6881280K ........ ........ ........ ........ 76.1M\n",
            "6914048K ........ ........ ........ ........ 59.8M\n",
            "6946816K ........ ........ ........ ........  120M\n",
            "6979584K ........ ........ ........ ........ 90.1M\n",
            "7012352K ........ ........ ........ ........  106M\n",
            "7045120K ........ ........ ........ ........ 91.0M\n",
            "7077888K ........ ........ ........ ........  127M\n",
            "7110656K ........ ........ ........ ........ 85.2M\n",
            "7143424K ........ ........ ........ ........  104M\n",
            "7176192K ........ ........ ........ ........ 50.7M\n",
            "7208960K ........ ........ ........ ........ 30.9M\n",
            "7241728K ........ ........ ........ ........ 51.5M\n",
            "7274496K ........ ........ ........ ........ 31.6M\n",
            "7307264K ........ ........ ........ ........ 31.3M\n",
            "7340032K ........ ........ ........ ........ 25.2M\n",
            "7372800K ........ ........ ........ ........ 12.2M\n",
            "7405568K ........ ........ ........ ........ 43.7M\n",
            "7438336K ........ ........ ........ ........ 33.6M\n",
            "7471104K ........ ........ ........ ........ 49.4M\n",
            "7503872K ........ ........ ........ ........ 40.0M\n",
            "7536640K ........ ........ ........ ........ 28.2M\n",
            "7569408K ........ ........ ........ ........ 59.9M\n",
            "7602176K ........ ........ ........ ........ 7.89M\n",
            "7634944K ........ ........ ........ ........ 63.0M\n",
            "7667712K ........ ........ ........ ........ 32.7M\n",
            "7700480K ........ ........ ........ ........ 70.9M\n",
            "7733248K ........ ........ ........ ........ 57.4M\n",
            "7766016K ........ ........ ........ ........ 71.7M\n",
            "7798784K ........ ........ ........ ........ 89.3M\n",
            "7831552K ........ ........ ........ ........ 80.3M\n",
            "7864320K ........ ........ ........ ........ 72.6M\n",
            "7897088K ........ ........ ........ ........  129M\n",
            "7929856K ........ ........ ........ ........  185M\n",
            "7962624K ........ ........ ........ ........  175M\n",
            "7995392K ........ ........ ........ ........ 37.2M\n",
            "8028160K ........ ........ ........ ........  238M\n",
            "8060928K ........ ........ ........ ........  127M\n",
            "8093696K ........ ........ ........ ........ 30.0M\n",
            "8126464K ........ ........ ........ ........ 30.6M\n",
            "8159232K ........ ........ ........ ........ 31.4M\n",
            "8192000K ........ ........ ........ ........ 16.0M\n",
            "8224768K ........ ........ ........ ........ 25.0M\n",
            "8257536K ........ ........ ........ ........ 24.9M\n",
            "8290304K ........ ........ ........ ........ 63.0M\n",
            "8323072K ........ ........ ........ ........ 52.3M\n",
            "8355840K ........ ........ ........ ........ 39.4M\n",
            "8388608K ........ ........ ........ ........ 24.3M\n",
            "8421376K ........ ........ ........ ........ 38.5M\n",
            "8454144K ........ ........ ........ ........ 38.7M\n",
            "8486912K ........ ........ ........ ........ 28.4M\n",
            "8519680K ........ ........ ........ ........ 61.7M\n",
            "8552448K ........ ........ ........ ........ 44.8M\n",
            "8585216K ........ ........ ........ ........ 32.3M\n",
            "8617984K ........ ........ ........ ........ 29.2M\n",
            "8650752K ........ ........ ........ ........ 85.6M\n",
            "8683520K ........ ........ ........ ........ 90.4M\n",
            "8716288K ........ ........ ........ ........ 46.0M\n",
            "8749056K ........ ........ ........ ........ 32.9M\n",
            "8781824K ........ ........ ........ ........ 22.2M\n",
            "8814592K ........ ........ ........ ........ 31.1M\n",
            "8847360K ........ ........ ........ ........ 46.6M\n",
            "8880128K ........ ........ ........ ........ 33.3M\n",
            "8912896K ........ ........ ........ ........ 34.9M\n",
            "8945664K ...                                 65.7M=3m19s\n",
            "\n",
            "2020-07-06 09:17:42 (44.0 MB/s) - ‘gdown.20200706091422.977766455457644’ saved [9163696640]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VCdEMpnUPqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv gdown.20* lung.tar"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxyzgNzUl7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf lung.tar"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKxwnstk4s2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm lung.tar"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV5alv7hUnz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "1d7c9aee-ce6d-4481-ecc9-1099ce78b5ff"
      },
      "source": [
        "!pip install git+git://github.com/s-mostafa-a/niftidataset.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/s-mostafa-a/niftidataset.git\n",
            "  Cloning git://github.com/s-mostafa-a/niftidataset.git to /tmp/pip-req-build-3rtuoo4v\n",
            "  Running command git clone -q git://github.com/s-mostafa-a/niftidataset.git /tmp/pip-req-build-3rtuoo4v\n",
            "Requirement already satisfied: nibabel>=2.3.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.18.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->niftidataset==0.2.0) (0.16.0)\n",
            "Building wheels for collected packages: niftidataset\n",
            "  Building wheel for niftidataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for niftidataset: filename=niftidataset-0.2.0-cp36-none-any.whl size=11682 sha256=34e31908f97c521703c4477d2912ab17fc5f3110328c9001a8c1fe3d2451900b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dr7bs2nr/wheels/90/80/c0/09c0a7dbe8a2902eeca8ebc007b7cbe2b5a06389d3f71d54f9\n",
            "Successfully built niftidataset\n",
            "Installing collected packages: niftidataset\n",
            "Successfully installed niftidataset-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5MEdP7lUtq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from niftidataset import *\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import fastai\n",
        "from fastai import vision\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms \n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gOkVmPCsFeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def conv3d(in_channels, out_channels, kernel_size, bias, padding):\n",
        "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
        "\n",
        "\n",
        "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
        "    \"\"\"\n",
        "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
        "    and optional batchnorm/groupnorm.\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size(int or tuple): size of the convolving kernel\n",
        "        order (string): order of things, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'gcr' -> groupnorm + conv + ReLU\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "            'bcr' -> batchnorm + conv + ReLU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    Return:\n",
        "        list of tuple (name, module)\n",
        "    \"\"\"\n",
        "    assert 'c' in order, \"Conv layer MUST be present\"\n",
        "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
        "\n",
        "    modules = []\n",
        "    for i, char in enumerate(order):\n",
        "        if char == 'r':\n",
        "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
        "        elif char == 'l':\n",
        "            modules.append(('LeakyReLU', nn.LeakyReLU(negative_slope=0.1, inplace=True)))\n",
        "        elif char == 'e':\n",
        "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
        "        elif char == 'c':\n",
        "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
        "            bias = not ('g' in order or 'b' in order)\n",
        "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
        "        elif char == 'g':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is_before_conv:\n",
        "                num_channels = in_channels\n",
        "            else:\n",
        "                num_channels = out_channels\n",
        "\n",
        "            # use only one group if the given number of groups is greater than the number of channels\n",
        "            if num_channels < num_groups:\n",
        "                num_groups = 1\n",
        "\n",
        "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
        "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
        "        elif char == 'b':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is_before_conv:\n",
        "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
        "            else:\n",
        "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
        "\n",
        "    return modules\n",
        "\n",
        "\n",
        "class SingleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
        "    of operations can be specified via the `order` parameter\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple):\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
        "        super(SingleConv, self).__init__()\n",
        "\n",
        "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
        "            self.add_module(name, module)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
        "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
        "    This can be changed however by providing the 'order' argument, e.g. in order\n",
        "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
        "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
        "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        if encoder:\n",
        "            # we're in the encoder path\n",
        "            conv1_in_channels = in_channels\n",
        "            conv1_out_channels = out_channels // 2\n",
        "            if conv1_out_channels < in_channels:\n",
        "                conv1_out_channels = in_channels\n",
        "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
        "        else:\n",
        "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
        "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
        "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
        "\n",
        "        # conv1\n",
        "        self.add_module('SingleConv1',\n",
        "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding))\n",
        "        # conv2\n",
        "        self.add_module('SingleConv2',\n",
        "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding))\n",
        "\n",
        "\n",
        "class ExtResNetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic UNet block consisting of a SingleConv followed by the residual block.\n",
        "    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n",
        "    of output channels is compatible with the residual block that follows.\n",
        "    This block can be used instead of standard DoubleConv in the Encoder module.\n",
        "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
        "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, **kwargs):\n",
        "        super(ExtResNetBlock, self).__init__()\n",
        "\n",
        "        # first convolution\n",
        "        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
        "        # residual block\n",
        "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
        "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
        "        n_order = order\n",
        "        for c in 'rel':\n",
        "            n_order = n_order.replace(c, '')\n",
        "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
        "                                num_groups=num_groups)\n",
        "\n",
        "        # create non-linearity separately\n",
        "        if 'l' in order:\n",
        "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        elif 'e' in order:\n",
        "            self.non_linearity = nn.ELU(inplace=True)\n",
        "        else:\n",
        "            self.non_linearity = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply first convolution and save the output as a residual\n",
        "        out = self.conv1(x)\n",
        "        residual = out\n",
        "\n",
        "        # residual block\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.non_linearity(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module from the encoder path consisting of the optional max\n",
        "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
        "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
        "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
        "    a DoubleConv module.\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        pool_type (str): pooling layer: 'max' or 'avg'\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
        "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
        "                 num_groups=8, padding=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        assert pool_type in ['max', 'avg']\n",
        "        if apply_pooling:\n",
        "            if pool_type == 'max':\n",
        "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
        "            else:\n",
        "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
        "        else:\n",
        "            self.pooling = None\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=True,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pooling is not None:\n",
        "            x = self.pooling(x)\n",
        "        x = self.basic_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module for decoder path consisting of the upsampling layer\n",
        "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
        "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
        "            from the corresponding encoder\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
        "                 conv_layer_order='gcr', num_groups=8, mode='nearest', padding=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        if basic_module == DoubleConv:\n",
        "            # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
        "            self.upsampling = Upsampling(transposed_conv=False, in_channels=in_channels, out_channels=out_channels,\n",
        "                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n",
        "            # concat joining\n",
        "            self.joining = partial(self._joining, concat=True)\n",
        "        else:\n",
        "            # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
        "            self.upsampling = Upsampling(transposed_conv=True, in_channels=in_channels, out_channels=out_channels,\n",
        "                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n",
        "            # sum joining\n",
        "            self.joining = partial(self._joining, concat=False)\n",
        "            # adapt the number of in_channels for the ExtResNetBlock\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=False,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding)\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
        "        x = self.joining(encoder_features, x)\n",
        "        x = self.basic_module(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _joining(encoder_features, x, concat):\n",
        "        if concat:\n",
        "            return torch.cat((encoder_features, x), dim=1)\n",
        "        else:\n",
        "            return encoder_features + x\n",
        "\n",
        "\n",
        "class Upsampling(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsamples a given multi-channel 3D data using either interpolation or learned transposed convolution.\n",
        "    Args:\n",
        "        transposed_conv (bool): if True uses ConvTranspose3d for upsampling, otherwise uses interpolation\n",
        "        in_channels (int): number of input channels for transposed conv\n",
        "            used only if transposed_conv is True\n",
        "        out_channels (int): number of output channels for transpose conv\n",
        "            used only if transposed_conv is True\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "            used only if transposed_conv is True\n",
        "        scale_factor (int or tuple): stride of the convolution\n",
        "            used only if transposed_conv is True\n",
        "        mode (str): algorithm used for upsampling:\n",
        "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
        "            used only if transposed_conv is False\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transposed_conv, in_channels=None, out_channels=None, kernel_size=3,\n",
        "                 scale_factor=(2, 2, 2), mode='nearest'):\n",
        "        super(Upsampling, self).__init__()\n",
        "\n",
        "        if transposed_conv:\n",
        "            # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
        "            # (D_out = (D_in − 1) ×  stride[0] − 2 ×  padding[0] +  kernel_size[0] +  output_padding[0])\n",
        "            self.upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
        "                                               padding=1)\n",
        "        else:\n",
        "            self.upsample = partial(self._interpolate, mode=mode)\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        output_size = encoder_features.size()[2:]\n",
        "        return self.upsample(x, output_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def _interpolate(x, size, mode):\n",
        "        return F.interpolate(x, size=size, mode=mode)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEJpRZLRsDx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import importlib\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "def number_of_features_per_level(init_channel_number, num_levels):\n",
        "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
        "\n",
        "\n",
        "class Abstract3DUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for standard and residual UNet.\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output segmentation masks;\n",
        "            Note that that the of out_channels might correspond to either\n",
        "            different semantic classes or to different binary segmentation mask.\n",
        "            It's up to the user of the class to interpret the out_channels and\n",
        "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
        "            or BCEWithLogitsLoss (two-class) respectively)\n",
        "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
        "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
        "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
        "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
        "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
        "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
        "        layer_order (string): determines the order of layers\n",
        "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
        "            See `SingleConv` for more info\n",
        "        f_maps (int, tuple): if int: number of feature maps in the first conv layer of the encoder (default: 64);\n",
        "            if tuple: number of feature maps at each level\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
        "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
        "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
        "        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)\n",
        "            will be applied as the last operation during the forward pass; if False the model is in training mode\n",
        "            and the `final_activation` (even if present) won't be applied; default: False\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, testing=False,\n",
        "                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):\n",
        "        super(Abstract3DUNet, self).__init__()\n",
        "\n",
        "        self.testing = testing\n",
        "\n",
        "        if isinstance(f_maps, int):\n",
        "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
        "\n",
        "        # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
        "        encoders = []\n",
        "        for i, out_feature_num in enumerate(f_maps):\n",
        "            if i == 0:\n",
        "                encoder = Encoder(in_channels, out_feature_num,\n",
        "                                  apply_pooling=False,  # skip pooling in the firs encoder\n",
        "                                  basic_module=basic_module,\n",
        "                                  conv_layer_order=layer_order,\n",
        "                                  conv_kernel_size=conv_kernel_size,\n",
        "                                  num_groups=num_groups,\n",
        "                                  padding=conv_padding)\n",
        "            else:\n",
        "                # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
        "                encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
        "                                  basic_module=basic_module,\n",
        "                                  conv_layer_order=layer_order,\n",
        "                                  conv_kernel_size=conv_kernel_size,\n",
        "                                  num_groups=num_groups,\n",
        "                                  pool_kernel_size=pool_kernel_size,\n",
        "                                  padding=conv_padding)\n",
        "\n",
        "            encoders.append(encoder)\n",
        "\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        # create decoder path consisting of the Decoder modules. The length of the decoder is equal to `len(f_maps) - 1`\n",
        "        decoders = []\n",
        "        reversed_f_maps = list(reversed(f_maps))\n",
        "        for i in range(len(reversed_f_maps) - 1):\n",
        "            if basic_module == DoubleConv:\n",
        "                in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
        "            else:\n",
        "                in_feature_num = reversed_f_maps[i]\n",
        "\n",
        "            out_feature_num = reversed_f_maps[i + 1]\n",
        "            # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
        "            # currently strides with a constant stride: (2, 2, 2)\n",
        "            decoder = Decoder(in_feature_num, out_feature_num,\n",
        "                              basic_module=basic_module,\n",
        "                              conv_layer_order=layer_order,\n",
        "                              conv_kernel_size=conv_kernel_size,\n",
        "                              num_groups=num_groups,\n",
        "                              padding=conv_padding)\n",
        "            decoders.append(decoder)\n",
        "\n",
        "        self.decoders = nn.ModuleList(decoders)\n",
        "\n",
        "        # in the last layer a 1×1 convolution reduces the number of output\n",
        "        # channels to the number of labels\n",
        "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
        "\n",
        "        if is_segmentation:\n",
        "            # semantic segmentation problem\n",
        "            if final_sigmoid:\n",
        "                self.final_activation = nn.Sigmoid()\n",
        "            else:\n",
        "                self.final_activation = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            # regression problem\n",
        "            self.final_activation = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder part\n",
        "        encoders_features = []\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "            # reverse the encoder outputs to be aligned with the decoder\n",
        "            encoders_features.insert(0, x)\n",
        "\n",
        "        # remove the last encoder's output from the list\n",
        "        # !!remember: it's the 1st in the list\n",
        "        encoders_features = encoders_features[1:]\n",
        "\n",
        "        # decoder part\n",
        "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
        "            # pass the output from the corresponding encoder and the output\n",
        "            # of the previous decoder\n",
        "            x = decoder(encoder_features, x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs\n",
        "        # logits and it's up to the user to normalize it before visualising with tensorboard or computing validation metric\n",
        "        if self.testing and self.final_activation is not None:\n",
        "            x = self.final_activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet3D(Abstract3DUNet):\n",
        "    \"\"\"\n",
        "    3DUnet model from\n",
        "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
        "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
        "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
        "        super(UNet3D, self).__init__(in_channels=in_channels, out_channels=out_channels, final_sigmoid=final_sigmoid,\n",
        "                                     basic_module=DoubleConv, f_maps=f_maps, layer_order=layer_order,\n",
        "                                     num_groups=num_groups, num_levels=num_levels, is_segmentation=is_segmentation,\n",
        "                                     conv_padding=conv_padding, **kwargs)\n",
        "\n",
        "\n",
        "class ResidualUNet3D(Abstract3DUNet):\n",
        "    \"\"\"\n",
        "    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
        "    Uses ExtResNetBlock as a basic building block, summation joining instead\n",
        "    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n",
        "    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1, **kwargs):\n",
        "        super(ResidualUNet3D, self).__init__(in_channels=in_channels, out_channels=out_channels,\n",
        "                                             final_sigmoid=final_sigmoid,\n",
        "                                             basic_module=ExtResNetBlock, f_maps=f_maps, layer_order=layer_order,\n",
        "                                             num_groups=num_groups, num_levels=num_levels,\n",
        "                                             is_segmentation=is_segmentation, conv_padding=conv_padding,\n",
        "                                             **kwargs)\n",
        "\n",
        "\n",
        "class UNet2D(Abstract3DUNet):\n",
        "    \"\"\"\n",
        "    Just a standard 2D Unet. Arises naturally by specifying conv_kernel_size=(1, 3, 3), pool_kernel_size=(1, 2, 2).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
        "        if conv_padding == 1:\n",
        "            conv_padding = (0, 1, 1)\n",
        "        super(UNet2D, self).__init__(in_channels=in_channels,\n",
        "                                     out_channels=out_channels,\n",
        "                                     final_sigmoid=final_sigmoid,\n",
        "                                     basic_module=DoubleConv,\n",
        "                                     f_maps=f_maps,\n",
        "                                     layer_order=layer_order,\n",
        "                                     num_groups=num_groups,\n",
        "                                     num_levels=num_levels,\n",
        "                                     is_segmentation=is_segmentation,\n",
        "                                     conv_kernel_size=(1, 3, 3),\n",
        "                                     pool_kernel_size=(1, 2, 2),\n",
        "                                     conv_padding=conv_padding,\n",
        "                                     **kwargs)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOt5AFW8r5C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_as_one_hot(input, C, ignore_index=None):\n",
        "    \"\"\"\n",
        "    Converts NxDxHxW label image to NxCxDxHxW, where each label gets converted to its corresponding one-hot vector\n",
        "    :param input: 4D input image (NxDxHxW)\n",
        "    :param C: number of channels/labels\n",
        "    :param ignore_index: ignore index to be kept during the expansion\n",
        "    :return: 5D output image (NxCxDxHxW)\n",
        "    \"\"\"\n",
        "    assert input.dim() == 4\n",
        "\n",
        "    # expand the input tensor to Nx1xDxHxW before scattering\n",
        "    input = input.unsqueeze(1)\n",
        "    # create result tensor shape (NxCxDxHxW)\n",
        "    shape = list(input.size())\n",
        "    shape[1] = C\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        # create ignore_index mask for the result\n",
        "        mask = input.expand(shape) == ignore_index\n",
        "        # clone the src tensor and zero out ignore_index in the input\n",
        "        input = input.clone()\n",
        "        input[input == ignore_index] = 0\n",
        "        # scatter to get the one-hot tensor\n",
        "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
        "        # bring back the ignore_index in the result\n",
        "        result[mask] = ignore_index\n",
        "        return result\n",
        "    else:\n",
        "        # scatter to get the one-hot tensor\n",
        "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbLHxI3BryUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## losses\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of contrastive loss defined in https://arxiv.org/pdf/1708.02551.pdf\n",
        "    'Semantic Instance Segmentation with a Discriminative Loss Function'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delta_var, delta_dist, norm='fro', alpha=1., beta=1., gamma=0.001):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.delta_var = delta_var\n",
        "        self.delta_dist = delta_dist\n",
        "        self.norm = norm\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def _compute_cluster_means(self, input, target):\n",
        "        embedding_dims = input.size()[1]\n",
        "        # expand target: NxCxDxHxW -> NxCxExDxHxW\n",
        "        # NxCx1xDxHxW\n",
        "        target = target.unsqueeze(2)\n",
        "        # save target's copy in order to compute the average embeddings later\n",
        "        target_copy = target.clone()\n",
        "        shape = list(target.size())\n",
        "        shape[2] = embedding_dims\n",
        "        target = target.expand(shape)\n",
        "\n",
        "        # expand input: NxExDxHxW -> Nx1xExDxHxW\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        # sum embeddings in each instance (multiply first via broadcasting) output: NxCxEx1x1x1\n",
        "        embeddings_per_instance = input * target\n",
        "        num = torch.sum(embeddings_per_instance, dim=(3, 4, 5), keepdim=True)\n",
        "        # get number of voxels in each cluster output: NxCx1x1x1x1\n",
        "        num_voxels_per_instance = torch.sum(target_copy, dim=(3, 4, 5), keepdim=True)\n",
        "        # compute mean embeddings NxCxEx1x1x1\n",
        "        mean_embeddings = num / num_voxels_per_instance\n",
        "        # return mean embeddings and additional tensors needed for further computations\n",
        "        return mean_embeddings, embeddings_per_instance\n",
        "\n",
        "    def _compute_variance_term(self, cluster_means, embeddings_per_instance, target):\n",
        "        # compute the distance to cluster means, result:(NxCxDxHxW)\n",
        "        embedding_norms = torch.norm(embeddings_per_instance - cluster_means, self.norm, dim=2)\n",
        "        # get per instance distances (apply instance mask)\n",
        "        embedding_norms = embedding_norms * target\n",
        "        # zero out distances less than delta_var and sum to get the variance (NxC)\n",
        "        embedding_variance = torch.clamp(embedding_norms - self.delta_var, min=0) ** 2\n",
        "        embedding_variance = torch.sum(embedding_variance, dim=(2, 3, 4))\n",
        "        # get number of voxels per instance (NxC)\n",
        "        num_voxels_per_instance = torch.sum(target, dim=(2, 3, 4))\n",
        "        # normalize the variance term\n",
        "        C = target.size()[1]\n",
        "        variance_term = torch.sum(embedding_variance / num_voxels_per_instance, dim=1) / C\n",
        "        return variance_term\n",
        "\n",
        "    def _compute_distance_term(self, cluster_means, C):\n",
        "        if C == 1:\n",
        "            # just one cluster in the batch, so distance term does not contribute to the loss\n",
        "            return 0.\n",
        "        # squeeze space dims\n",
        "        for _ in range(3):\n",
        "            cluster_means = cluster_means.squeeze(-1)\n",
        "        # expand cluster_means tensor in order to compute the pair-wise distance between cluster means\n",
        "        cluster_means = cluster_means.unsqueeze(1)\n",
        "        shape = list(cluster_means.size())\n",
        "        shape[1] = C\n",
        "        # NxCxCxEx1x1x1\n",
        "        cm_matrix1 = cluster_means.expand(shape)\n",
        "        # transpose the cluster_means matrix in order to compute pair-wise distances\n",
        "        cm_matrix2 = cm_matrix1.permute(0, 2, 1, 3)\n",
        "        # compute pair-wise distances (NxCxC)\n",
        "        dist_matrix = torch.norm(cm_matrix1 - cm_matrix2, p=self.norm, dim=3)\n",
        "        # create matrix for the repulsion distance (i.e. cluster centers further apart than 2 * delta_dist\n",
        "        # are not longer repulsed)\n",
        "        repulsion_dist = 2 * self.delta_dist * (1 - torch.eye(C))\n",
        "        # 1xCxC\n",
        "        repulsion_dist = repulsion_dist.unsqueeze(0).to(cluster_means.device)\n",
        "        # zero out distances grater than 2*delta_dist (NxCxC)\n",
        "        hinged_dist = torch.clamp(repulsion_dist - dist_matrix, min=0) ** 2\n",
        "        # sum all of the hinged pair-wise distances\n",
        "        hinged_dist = torch.sum(hinged_dist, dim=(1, 2))\n",
        "        # normalized by the number of paris and return\n",
        "        return hinged_dist / (C * (C - 1))\n",
        "\n",
        "    def _compute_regularizer_term(self, cluster_means, C):\n",
        "        # squeeze space dims\n",
        "        for _ in range(3):\n",
        "            cluster_means = cluster_means.squeeze(-1)\n",
        "        norms = torch.norm(cluster_means, p=self.norm, dim=2)\n",
        "        assert norms.size()[1] == C\n",
        "        # return the average norm per batch\n",
        "        return torch.sum(norms, dim=1).div(C)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "             input (torch.tensor): embeddings predicted by the network (NxExDxHxW) (E - embedding dims)\n",
        "             target (torch.tensor): ground truth instance segmentation (NxDxHxW)\n",
        "        Returns:\n",
        "            Combined loss defined as: alpha * variance_term + beta * distance_term + gamma * regularization_term\n",
        "        \"\"\"\n",
        "        # get number of instances in the batch\n",
        "        C = torch.unique(target).size()[0]\n",
        "        # expand each label as a one-hot vector: N x D x H x W -> N x C x D x H x W\n",
        "        target = expand_as_one_hot(target, C)\n",
        "        # compare spatial dimensions\n",
        "        assert input.dim() == target.dim() == 5\n",
        "        assert input.size()[2:] == target.size()[2:]\n",
        "\n",
        "        # compute mean embeddings and assign embeddings to instances\n",
        "        cluster_means, embeddings_per_instance = self._compute_cluster_means(input, target)\n",
        "        variance_term = self._compute_variance_term(cluster_means, embeddings_per_instance, target)\n",
        "        distance_term = self._compute_distance_term(cluster_means, C)\n",
        "        regularization_term = self._compute_regularizer_term(cluster_means, C)\n",
        "        # total loss\n",
        "        loss = self.alpha * variance_term + self.beta * distance_term + self.gamma * regularization_term\n",
        "        # reduce batch dimension\n",
        "        return torch.mean(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import MSELoss, SmoothL1Loss, L1Loss\n",
        "\n",
        "\n",
        "def compute_per_channel_dice(input, target, epsilon=1e-6, weight=None):\n",
        "    \"\"\"\n",
        "    Computes DiceCoefficient as defined in https://arxiv.org/abs/1606.04797 given  a multi channel input and target.\n",
        "    Assumes the input is a normalized probability, e.g. a result of Sigmoid or Softmax function.\n",
        "    Args:\n",
        "         input (torch.Tensor): NxCxSpatial input tensor\n",
        "         target (torch.Tensor): NxCxSpatial target tensor\n",
        "         epsilon (float): prevents division by zero\n",
        "         weight (torch.Tensor): Cx1 tensor of weight per channel/class\n",
        "    \"\"\"\n",
        "\n",
        "    # input and target shapes must match\n",
        "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
        "\n",
        "    input = flatten(input)\n",
        "    target = flatten(target)\n",
        "    target = target.float()\n",
        "\n",
        "    # compute per channel Dice Coefficient\n",
        "    intersect = (input * target).sum(-1)\n",
        "    if weight is not None:\n",
        "        intersect = weight * intersect\n",
        "\n",
        "    # here we can use standard dice (input + target).sum(-1) or extension (see V-Net) (input^2 + target^2).sum(-1)\n",
        "    denominator = (input * input).sum(-1) + (target * target).sum(-1)\n",
        "    return 2 * (intersect / denominator.clamp(min=epsilon))\n",
        "\n",
        "\n",
        "class _MaskingLossWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss wrapper which prevents the gradient of the loss to be computed where target is equal to `ignore_index`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss, ignore_index):\n",
        "        super(_MaskingLossWrapper, self).__init__()\n",
        "        assert ignore_index is not None, 'ignore_index cannot be None'\n",
        "        self.loss = loss\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        mask = target.clone().ne_(self.ignore_index)\n",
        "        mask.requires_grad = False\n",
        "\n",
        "        # mask out input/target so that the gradient is zero where on the mask\n",
        "        input = input * mask\n",
        "        target = target * mask\n",
        "\n",
        "        # forward masked input and target to the loss\n",
        "        return self.loss(input, target)\n",
        "\n",
        "\n",
        "class SkipLastTargetChannelWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss wrapper which removes additional target channel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss, squeeze_channel=False):\n",
        "        super(SkipLastTargetChannelWrapper, self).__init__()\n",
        "        self.loss = loss\n",
        "        self.squeeze_channel = squeeze_channel\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        assert target.size(1) > 1, 'Target tensor has a singleton channel dimension, cannot remove channel'\n",
        "\n",
        "        # skips last target channel if needed\n",
        "        target = target[:, :-1, ...]\n",
        "\n",
        "        if self.squeeze_channel:\n",
        "            # squeeze channel dimension if singleton\n",
        "            target = torch.squeeze(target, dim=1)\n",
        "        return self.loss(input, target)\n",
        "\n",
        "\n",
        "class _AbstractDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for different implementations of Dice loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, sigmoid_normalization=True):\n",
        "        super(_AbstractDiceLoss, self).__init__()\n",
        "        self.register_buffer('weight', weight)\n",
        "        # The output from the network during training is assumed to be un-normalized probabilities and we would\n",
        "        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n",
        "        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n",
        "        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n",
        "        # output, just specify sigmoid_normalization=False.\n",
        "        if sigmoid_normalization:\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        else:\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        # actual Dice score computation; to be implemented by the subclass\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # get probabilities from logits\n",
        "        input = self.normalization(input)\n",
        "\n",
        "        # compute per channel Dice coefficient\n",
        "        per_channel_dice = self.dice(input, target, weight=self.weight)\n",
        "\n",
        "        # average Dice score across all channels/classes\n",
        "        return 1. - torch.mean(per_channel_dice)\n",
        "\n",
        "\n",
        "class DiceLoss(_AbstractDiceLoss):\n",
        "    \"\"\"Computes Dice Loss according to https://arxiv.org/abs/1606.04797.\n",
        "    For multi-class segmentation `weight` parameter can be used to assign different weights per class.\n",
        "    The input to the loss function is assumed to be a logit and will be normalized by the Sigmoid function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, sigmoid_normalization=True):\n",
        "        super().__init__(weight, sigmoid_normalization)\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        return compute_per_channel_dice(input, target, weight=self.weight)\n",
        "\n",
        "\n",
        "class GeneralizedDiceLoss(_AbstractDiceLoss):\n",
        "    \"\"\"Computes Generalized Dice Loss (GDL) as described in https://arxiv.org/pdf/1707.03237.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigmoid_normalization=True, epsilon=1e-6):\n",
        "        super().__init__(weight=None, sigmoid_normalization=sigmoid_normalization)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
        "\n",
        "        input = flatten(input)\n",
        "        target = flatten(target)\n",
        "        target = target.float()\n",
        "\n",
        "        if input.size(0) == 1:\n",
        "            # for GDL to make sense we need at least 2 channels (see https://arxiv.org/pdf/1707.03237.pdf)\n",
        "            # put foreground and background voxels in separate channels\n",
        "            input = torch.cat((input, 1 - input), dim=0)\n",
        "            target = torch.cat((target, 1 - target), dim=0)\n",
        "\n",
        "        # GDL weighting: the contribution of each label is corrected by the inverse of its volume\n",
        "        w_l = target.sum(-1)\n",
        "        w_l = 1 / (w_l * w_l).clamp(min=self.epsilon)\n",
        "        w_l.requires_grad = False\n",
        "\n",
        "        intersect = (input * target).sum(-1)\n",
        "        intersect = intersect * w_l\n",
        "\n",
        "        denominator = (input + target).sum(-1)\n",
        "        denominator = (denominator * w_l).clamp(min=self.epsilon)\n",
        "\n",
        "        return 2 * (intersect.sum() / denominator.sum())\n",
        "\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    \"\"\"Linear combination of BCE and Dice losses\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, beta):\n",
        "        super(BCEDiceLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.beta = beta\n",
        "        self.dice = DiceLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.alpha * self.bce(input, target) + self.beta * self.dice(input, target)\n",
        "\n",
        "\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    \"\"\"WeightedCrossEntropyLoss (WCE) as described in https://arxiv.org/pdf/1707.03237.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-1):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        weight = self._class_weights(input)\n",
        "        return F.cross_entropy(input, target, weight=weight, ignore_index=self.ignore_index)\n",
        "\n",
        "    @staticmethod\n",
        "    def _class_weights(input):\n",
        "        # normalize the input first\n",
        "        input = F.softmax(input, dim=1)\n",
        "        flattened = flatten(input)\n",
        "        nominator = (1. - flattened).sum(-1)\n",
        "        denominator = flattened.sum(-1)\n",
        "        class_weights = Variable(nominator / denominator, requires_grad=False)\n",
        "        return class_weights\n",
        "\n",
        "\n",
        "class PixelWiseCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, class_weights=None, ignore_index=None):\n",
        "        super(PixelWiseCrossEntropyLoss, self).__init__()\n",
        "        self.register_buffer('class_weights', class_weights)\n",
        "        self.ignore_index = ignore_index\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, target, weights):\n",
        "        assert target.size() == weights.size()\n",
        "        # normalize the input\n",
        "        log_probabilities = self.log_softmax(input)\n",
        "        # standard CrossEntropyLoss requires the target to be (NxDxHxW), so we need to expand it to (NxCxDxHxW)\n",
        "        target = expand_as_one_hot(target, C=input.size()[1], ignore_index=self.ignore_index)\n",
        "        # expand weights\n",
        "        weights = weights.unsqueeze(0)\n",
        "        weights = weights.expand_as(input)\n",
        "\n",
        "        # create default class_weights if None\n",
        "        if self.class_weights is None:\n",
        "            class_weights = torch.ones(input.size()[1]).float().to(input.device)\n",
        "        else:\n",
        "            class_weights = self.class_weights\n",
        "\n",
        "        # resize class_weights to be broadcastable into the weights\n",
        "        class_weights = class_weights.view(1, -1, 1, 1, 1)\n",
        "\n",
        "        # multiply weights tensor by class weights\n",
        "        weights = class_weights * weights\n",
        "\n",
        "        # compute the losses\n",
        "        result = -weights * target * log_probabilities\n",
        "        # average the losses\n",
        "        return result.mean()\n",
        "\n",
        "\n",
        "class TagsAngularLoss(nn.Module):\n",
        "    def __init__(self, tags_coefficients):\n",
        "        super(TagsAngularLoss, self).__init__()\n",
        "        self.tags_coefficients = tags_coefficients\n",
        "\n",
        "    def forward(self, inputs, targets, weight):\n",
        "        assert isinstance(inputs, list)\n",
        "        # if there is just one output head the 'inputs' is going to be a singleton list [tensor]\n",
        "        # and 'targets' is just going to be a tensor (that's how the HDF5Dataloader works)\n",
        "        # so wrap targets in a list in this case\n",
        "        if len(inputs) == 1:\n",
        "            targets = [targets]\n",
        "        assert len(inputs) == len(targets) == len(self.tags_coefficients)\n",
        "        loss = 0\n",
        "        for input, target, alpha in zip(inputs, targets, self.tags_coefficients):\n",
        "            loss += alpha * square_angular_loss(input, target, weight)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class WeightedSmoothL1Loss(nn.SmoothL1Loss):\n",
        "    def __init__(self, threshold, initial_weight, apply_below_threshold=True):\n",
        "        super().__init__(reduction=\"none\")\n",
        "        self.threshold = threshold\n",
        "        self.apply_below_threshold = apply_below_threshold\n",
        "        self.weight = initial_weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = super().forward(input, target)\n",
        "\n",
        "        if self.apply_below_threshold:\n",
        "            mask = target < self.threshold\n",
        "        else:\n",
        "            mask = target >= self.threshold\n",
        "\n",
        "        l1[mask] = l1[mask] * self.weight\n",
        "\n",
        "        return l1.mean()\n",
        "\n",
        "\n",
        "def square_angular_loss(input, target, weights=None):\n",
        "    \"\"\"\n",
        "    Computes square angular loss between input and target directions.\n",
        "    Makes sure that the input and target directions are normalized so that torch.acos would not produce NaNs.\n",
        "    :param input: 5D input tensor (NCDHW)\n",
        "    :param target: 5D target tensor (NCDHW)\n",
        "    :param weights: 3D weight tensor in order to balance different instance sizes\n",
        "    :return: per pixel weighted sum of squared angular losses\n",
        "    \"\"\"\n",
        "    assert input.size() == target.size()\n",
        "    # normalize and multiply by the stability_coeff in order to prevent NaN results from torch.acos\n",
        "    stability_coeff = 0.999999\n",
        "    input = input / torch.norm(input, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n",
        "    target = target / torch.norm(target, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n",
        "    # compute cosine map\n",
        "    cosines = (input * target).sum(dim=1)\n",
        "    error_radians = torch.acos(cosines)\n",
        "    if weights is not None:\n",
        "        return (error_radians * error_radians * weights).sum()\n",
        "    else:\n",
        "        return (error_radians * error_radians).sum()\n",
        "\n",
        "\n",
        "def flatten(tensor):\n",
        "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
        "    The shapes are transformed as follows:\n",
        "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
        "    \"\"\"\n",
        "    # number of channels\n",
        "    C = tensor.size(1)\n",
        "    # new axis order\n",
        "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
        "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
        "    transposed = tensor.permute(axis_order)\n",
        "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
        "    return transposed.contiguous().view(C, -1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VWqnVcSrtIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# metrics\n",
        "class MeanIoU:\n",
        "    \"\"\"\n",
        "    Computes IoU for each class separately and then averages over all classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, skip_channels=(), ignore_index=None, **kwargs):\n",
        "        \"\"\"\n",
        "        :param skip_channels: list/tuple of channels to be ignored from the IoU computation\n",
        "        :param ignore_index: id of the label to be ignored from IoU computation\n",
        "        \"\"\"\n",
        "        self.__name__ = 'MeanIoU'\n",
        "        self.ignore_index = ignore_index\n",
        "        self.skip_channels = skip_channels\n",
        "\n",
        "    def __call__(self, input, target):\n",
        "        \"\"\"\n",
        "        :param input: 5D probability maps torch float tensor (NxCxDxHxW)\n",
        "        :param target: 4D or 5D ground truth torch tensor. 4D (NxDxHxW) tensor will be expanded to 5D as one-hot\n",
        "        :return: intersection over union averaged over all channels\n",
        "        \"\"\"\n",
        "        assert input.dim() == 5\n",
        "\n",
        "        n_classes = input.size()[1]\n",
        "\n",
        "        if target.dim() == 4:\n",
        "            target = expand_as_one_hot(target, C=n_classes, ignore_index=self.ignore_index)\n",
        "\n",
        "        assert input.size() == target.size()\n",
        "\n",
        "        per_batch_iou = []\n",
        "        for _input, _target in zip(input, target):\n",
        "            binary_prediction = self._binarize_predictions(_input, n_classes)\n",
        "\n",
        "            if self.ignore_index is not None:\n",
        "                # zero out ignore_index\n",
        "                mask = _target == self.ignore_index\n",
        "                binary_prediction[mask] = 0\n",
        "                _target[mask] = 0\n",
        "\n",
        "            # convert to uint8 just in case\n",
        "            binary_prediction = binary_prediction.byte()\n",
        "            _target = _target.byte()\n",
        "\n",
        "            per_channel_iou = []\n",
        "            for c in range(n_classes):\n",
        "                if c in self.skip_channels:\n",
        "                    continue\n",
        "\n",
        "                per_channel_iou.append(self._jaccard_index(binary_prediction[c], _target[c]))\n",
        "\n",
        "            assert per_channel_iou, \"All channels were ignored from the computation\"\n",
        "            mean_iou = torch.mean(torch.tensor(per_channel_iou))\n",
        "            per_batch_iou.append(mean_iou)\n",
        "\n",
        "        return torch.mean(torch.tensor(per_batch_iou))\n",
        "\n",
        "    def _binarize_predictions(self, input, n_classes):\n",
        "        \"\"\"\n",
        "        Puts 1 for the class/channel with the highest probability and 0 in other channels. Returns byte tensor of the\n",
        "        same size as the input tensor.\n",
        "        \"\"\"\n",
        "        if n_classes == 1:\n",
        "            # for single channel input just threshold the probability map\n",
        "            result = input > 0.5\n",
        "            return result.long()\n",
        "\n",
        "        _, max_index = torch.max(input, dim=0, keepdim=True)\n",
        "        return torch.zeros_like(input, dtype=torch.uint8).scatter_(0, max_index, 1)\n",
        "\n",
        "    def _jaccard_index(self, prediction, target):\n",
        "        \"\"\"\n",
        "        Computes IoU for a given target and prediction tensors\n",
        "        \"\"\"\n",
        "        return torch.sum(prediction & target).float() / torch.clamp(torch.sum(prediction | target).float(), min=1e-8)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVWbkfFkRyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/Task06_Lung/processed"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiG4zW6M262w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nibabel as nib\n",
        "# from matplotlib.pyplot import imshow\n",
        "\n",
        "# f1 = '/content/Task06_Lung/imagesTr/lung_001.nii.gz'\n",
        "# f2 = '/content/Task06_Lung/processed/lung_001.nii'\n",
        "# f3 = '/content/Task06_Lung/labelsTr/lung_001.nii.gz'\n",
        "\n",
        "# imshow(nib.load(f1).get_fdata(dtype=np.float32)[:,:,100], cmap='gray')\n",
        "# imshow(nib.load(f2).get_fdata(dtype=np.float32)[:,:,100], cmap='gray')\n",
        "# imshow(nib.load(f3).get_fdata(dtype=np.float32)[:,:,100])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAVYNy55Wo7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nibabel as nib\n",
        "# image_tensor = None\n",
        "# path = '/content/Task06_Lung'\n",
        "# train_folder = 'imagesTr'\n",
        "# for fil_nam in glob_imgs(f'{path}/{train_folder}'):\n",
        "#   nib_data = nib.load(fil_nam)\n",
        "#   image_tensor = torch.as_tensor(nib_data.get_fdata(dtype=np.float32), dtype=torch.float32, device='cuda')\n",
        "#   mean = torch.as_tensor(image_tensor.mean(axis=2), dtype=torch.float32, device='cuda')\n",
        "#   std = torch.as_tensor(image_tensor.std(axis=2), dtype=torch.float32, device='cuda')\n",
        "#   std.add_(1)\n",
        "#   image_tensor.sub_(mean.unsqueeze(2)).div_(std.unsqueeze(2))\n",
        "#   print(image_tensor.mean(axis=2).sum(), fil_nam)\n",
        "#   processed_img = nib.Nifti1Image(image_tensor.cpu().numpy(), nib_data.affine, nib_data.header)\n",
        "#   nib.save(processed_img, f'''{path}/processed/{fil_nam.split('/')[-1].split('.')[0]}.nii''')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv6NDIDGUvfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a74bd4b8-2909-4cce-a302-5fc7d83b47ee"
      },
      "source": [
        "train_dir = '/content/Task06_Lung/'\n",
        "patch_sz = 64\n",
        "tfms = transforms.Compose([RandomCrop3D(patch_sz), ToTensor(), Normalize(mean=[0],std=[1],is_3d=True)])\n",
        "tds, vds = get_train_and_validation_from_one_directory(source_dir=train_dir+'imagesTr', target_dir=train_dir+'labelsTr', valid_pct=0.2, transform=tfms)\n",
        "print(len(tds), len(vds))\n",
        "from fastai import vision as faiv\n",
        "idb = faiv.ImageDataBunch.create(tds, vds, bs=2, num_workers=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuXiBCftrGXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "1efee439-53b2-443a-c7d3-d97805872f29"
      },
      "source": [
        "model = UNet3D(in_channels=1, out_channels=1, final_sigmoid=False)\n",
        "learner = faiv.Learner(idb, model, loss_func=DiceLoss(), metrics=[MeanIoU()])\n",
        "learner.lr_find(start_lr=1e-10, end_lr=1000)\n",
        "learner.recorder.plot()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='3' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      75.00% [3/4 13:14<04:24]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>MeanIoU</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.998538</td>\n",
              "      <td>#na#</td>\n",
              "      <td>04:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.998171</td>\n",
              "      <td>#na#</td>\n",
              "      <td>04:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.997699</td>\n",
              "      <td>#na#</td>\n",
              "      <td>04:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='24' class='' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      96.00% [24/25 04:04<00:10 0.9987]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxNPUxAHcEYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}