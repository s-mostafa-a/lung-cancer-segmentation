{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "using_vnet_for_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-mostafa-a/lung-cancer-segmentation/blob/master/using_vnet_for_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqX8FtHFBUsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _cn_t_d():\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anQKAPFR0gVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _untar_data():\n",
        "  !tar -xf ./drive/My\\ Drive/Copy_Task06_Lung.tar -C ./"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIhnVsuZxyD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_to_drive():\n",
        "  !mkdir drive\n",
        "  _cn_t_d()\n",
        "  _untar_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To7cAfecER4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def install_all_needed_packages():\n",
        "  !pip uninstall niftidataset\n",
        "  !pip install git+git://github.com/s-mostafa-a/niftidataset.git\n",
        "  !pip install git+git://github.com/mattmacy/torchbiomed.git\n",
        "  !git clone https://github.com/s-mostafa-a/vnet.pytorch\n",
        "  !pip install hdbscan\n",
        "  !mv ./vnet.pytorch ./pvn\n",
        "  !git clone https://github.com/wolny/pytorch-3dunet.git\n",
        "  !mv ./pytorch-3dunet ./p3dun\n",
        "  !pip install SimpleITK\n",
        "  !pip install setproctitle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RIFgY_D1Efz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_databunch():\n",
        "  np.random.seed(1)\n",
        "  train_dir = '/content/Task06_Lung/'\n",
        "  tfms = tvt.Compose([ToTensor(), RandomCrop3D(110), TrimIntensity(-1000.0,1000.0,0.0,255.0), Normalize(is_3d=True)])\n",
        "  tds, vds = train_val_split(source_dir=train_dir+'imagesTr', target_dir=train_dir+'labelsTr', valid_pct=0.2, transform=tfms)\n",
        "  print(len(tds), len(vds))\n",
        "  idb = faiv.ImageDataBunch.create(tds, vds, bs=1, num_workers=1)\n",
        "  return idb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YiXgGF-cFe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_learner(idb):\n",
        "  model = VNet()\n",
        "  metric = metrics.DiceCoefficient()\n",
        "  metric.__name__ = 'DiceCoefficient'\n",
        "  learner = faiv.Learner(idb, model, loss_func=losses.BCEDiceLoss(alpha=0.3, beta=0.7), metrics=[metric], opt_func=torch.optim.Adam)\n",
        "  return learner\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JydO7JulNEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e9e2fa8-37c6-45a3-886b-8df6f9eb50e3"
      },
      "source": [
        "connect_to_drive()\n",
        "install_all_needed_packages()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive’: File exists\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting git+git://github.com/jcreinhold/niftidataset.git\n",
            "  Cloning git://github.com/jcreinhold/niftidataset.git to /tmp/pip-req-build-2xmvceou\n",
            "  Running command git clone -q git://github.com/jcreinhold/niftidataset.git /tmp/pip-req-build-2xmvceou\n",
            "Requirement already satisfied: nibabel>=2.3.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.18.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->niftidataset==0.2.0) (0.16.0)\n",
            "Building wheels for collected packages: niftidataset\n",
            "  Building wheel for niftidataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for niftidataset: filename=niftidataset-0.2.0-cp36-none-any.whl size=12356 sha256=a8bb8239838c59640f1d6fcb8a9084623c37359a19b2733fb57cda16fa6817f2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cbw306sx/wheels/3d/60/12/b5ae6069c6d1cf966d06eb7d8336abd8f826601c3f31f17c3e\n",
            "Successfully built niftidataset\n",
            "Installing collected packages: niftidataset\n",
            "Successfully installed niftidataset-0.2.0\n",
            "Collecting git+git://github.com/mattmacy/torchbiomed.git\n",
            "  Cloning git://github.com/mattmacy/torchbiomed.git to /tmp/pip-req-build-wtbjvb8a\n",
            "  Running command git clone -q git://github.com/mattmacy/torchbiomed.git /tmp/pip-req-build-wtbjvb8a\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchbiomed==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbiomed==0.0.1) (7.0.0)\n",
            "Building wheels for collected packages: torchbiomed\n",
            "  Building wheel for torchbiomed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchbiomed: filename=torchbiomed-0.0.1-py2.py3-none-any.whl size=10974 sha256=f9c522f48bbecc521559871d768738a421572a5d860970d6c0728ac89ab76bac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9669_qlc/wheels/0e/38/99/f0e32a294d9839ca398db5ae025fb8896cdbd6370cd51e89b9\n",
            "Successfully built torchbiomed\n",
            "Installing collected packages: torchbiomed\n",
            "Successfully installed torchbiomed-0.0.1\n",
            "Cloning into 'vnet.pytorch'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Total 196 (delta 0), reused 0 (delta 0), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (196/196), 1.11 MiB | 2.19 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "Collecting hdbscan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/2f/2423d844072f007a74214c1adc46260e45f034bb1679ccadfbb8a601f647/hdbscan-0.8.26.tar.gz (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 8.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.29.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.18.5)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.26-cp36-cp36m-linux_x86_64.whl size=2307226 sha256=2a4420456a055e9941bc95f1da31b00c123661aa1a46435949f2cb45849dbcaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/38/41/372f034d8abd271ef7787a681e0a47fc05d472683a7eb088ed\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.26\n",
            "Cloning into 'pytorch-3dunet'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (371/371), done.\u001b[K\n",
            "remote: Compressing objects: 100% (216/216), done.\u001b[K\n",
            "remote: Total 2723 (delta 236), reused 263 (delta 152), pack-reused 2352\u001b[K\n",
            "Receiving objects: 100% (2723/2723), 296.97 MiB | 26.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1801/1801), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31b40jQRnqfK",
        "colab_type": "text"
      },
      "source": [
        "Because 3d image size are in like this: (512,512,z) which z range is from 112 to 465, I thougth it would be good if I crop each 3D image from it's third axis (z) and fix that in a constant like 110. This way raw data loss becomes so little. for example if we have an image sized (512,512, 112) we would generate an image (512,512,110), Also if we had an image sized (512, 512, 373) we would generate 3 images with sizes (512,512,110) which it covers 330 out of 373 from z of original raw data.\n",
        "\n",
        "But when I used it with size (512,512,110), cuda went out of memory and it needed more RAM. Also I found some paper which they cropped this dataset's images to size (64,64,64)! which I thought their data loss would be so high. But anyway I went a way similar to their way and cropped original images to the size (110,110,110) which almost contains 8 times more data (and less data loss) from that paper.\n",
        "\n",
        "So I wrote below classes which use some features of [this code](https://github.com/jcreinhold/niftidataset). It used a single image 5 times and cropped each randomly, but I thought It will not be useful because np.random.seed() makes each time just one crop. officially it wouldn't help.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj-0vmFzJ3_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e6c5984-c631-4aa8-b715-d0e5a3f74f14"
      },
      "source": [
        "!rm -r pvn\n",
        "!rm -r pytorch-3dunet\n",
        "!rm -r p3dun"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'pytorch-3dunet': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4a3Gf5MOCyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de858510-0fb4-4a52-9958-c0ac6f2e0221"
      },
      "source": [
        "install_all_needed_packages()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling niftidataset-0.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/niftidataset-0.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/niftidataset/*\n",
            "Proceed (y/n)? n\n",
            "Collecting git+git://github.com/s-mostafa-a/niftidataset.git\n",
            "  Cloning git://github.com/s-mostafa-a/niftidataset.git to /tmp/pip-req-build-mm6dshkw\n",
            "  Running command git clone -q git://github.com/s-mostafa-a/niftidataset.git /tmp/pip-req-build-mm6dshkw\n",
            "Requirement already satisfied (use --upgrade to upgrade): niftidataset==0.2.0 from git+git://github.com/s-mostafa-a/niftidataset.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: nibabel>=2.3.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.18.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from niftidataset==0.2.0) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->niftidataset==0.2.0) (0.16.0)\n",
            "Building wheels for collected packages: niftidataset\n",
            "  Building wheel for niftidataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for niftidataset: filename=niftidataset-0.2.0-cp36-none-any.whl size=12420 sha256=332f349b9581a4f3412409ef167ae9fb1e25d43dc6be198ac04912c5827998c6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jguc2ei9/wheels/90/80/c0/09c0a7dbe8a2902eeca8ebc007b7cbe2b5a06389d3f71d54f9\n",
            "Successfully built niftidataset\n",
            "Collecting git+git://github.com/mattmacy/torchbiomed.git\n",
            "  Cloning git://github.com/mattmacy/torchbiomed.git to /tmp/pip-req-build-5gl5giv1\n",
            "  Running command git clone -q git://github.com/mattmacy/torchbiomed.git /tmp/pip-req-build-5gl5giv1\n",
            "Requirement already satisfied (use --upgrade to upgrade): torchbiomed==0.0.1 from git+git://github.com/mattmacy/torchbiomed.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbiomed==0.0.1) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchbiomed==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbiomed==0.0.1) (7.0.0)\n",
            "Building wheels for collected packages: torchbiomed\n",
            "  Building wheel for torchbiomed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchbiomed: filename=torchbiomed-0.0.1-py2.py3-none-any.whl size=10974 sha256=86bac9c5db2a92e976cc3cae063be98147719b3b337034ed31c0080972701a27\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d2l_mhaq/wheels/0e/38/99/f0e32a294d9839ca398db5ae025fb8896cdbd6370cd51e89b9\n",
            "Successfully built torchbiomed\n",
            "Cloning into 'vnet.pytorch'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 216 (delta 8), reused 16 (delta 5), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (216/216), 1.12 MiB | 2.60 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.6/dist-packages (0.8.26)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.29.20)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.15.1)\n",
            "Cloning into 'pytorch-3dunet'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (371/371), done.\u001b[K\n",
            "remote: Compressing objects: 100% (216/216), done.\u001b[K\n",
            "remote: Total 2723 (delta 236), reused 263 (delta 152), pack-reused 2352\u001b[K\n",
            "Receiving objects: 100% (2723/2723), 296.97 MiB | 14.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1801/1801), done.\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (1.2.4)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.6/dist-packages (1.1.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrVuHH5K1wPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "43017cde-aa71-4761-eaab-756dd26f9dfd"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./pvn/'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "print(sys.path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/pvn']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZbpVQ5LOLYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from niftidataset import NiftiDataset, train_val_split, RandomCrop3D, ToTensor, Normalize, TrimIntensity\n",
        "from torchvision import transforms as tvt\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import torch \n",
        "from fastai import vision as faiv\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1hfSSxOE0f6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "67b90e8b-031d-434d-ae09-fc55aec82cb9"
      },
      "source": [
        "from train import main\n",
        "np.random.seed(1)\n",
        "train_dir = '/content/Task06_Lung/'\n",
        "tfms = tvt.Compose([ToTensor(), RandomCrop3D(110), TrimIntensity(-1000.0,1000.0,0.0,255.0), Normalize(is_3d=True)])\n",
        "tds, vds = train_val_split(source_dir=train_dir+'imagesTr', target_dir=train_dir+'labelsTr', valid_pct=0.2, transform=tfms)\n",
        "main(tds, vds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build vnet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/pvn/train.py:56: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "  nn.init.kaiming_normal(m.weight)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  + Number of params: 45603934\n",
            "loading test set\n",
            "[0.       0.       0.       0.       ... 0.       0.       0.043228 0.      ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-44bb9f4cf108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomCrop3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrimIntensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'imagesTr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'labelsTr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/pvn/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(tds, vds)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnEpochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0madjust_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mis_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pvn/train.py\u001b[0m in \u001b[0;36mtrain_dice\u001b[0;34m(args, epoch, model, trainLoader, optimizer, trainF, weights)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbioloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# make_graph.save('/tmp/t.dot', loss.creator); assert(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pvn/vnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#     self.out_tr = OutputTransition(16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mout16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_tr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mout32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_tr32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mout64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_tr64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pvn/vnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         x16 = torch.cat((x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0),\n\u001b[1;32m     61\u001b[0m                          x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0)), 0)\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.70 GiB (GPU 0; 14.73 GiB total capacity; 2.65 GiB already allocated; 11.28 GiB free; 2.68 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}